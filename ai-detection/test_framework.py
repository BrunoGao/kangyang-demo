#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜é›…çš„è·Œå€’æ£€æµ‹æµ‹è¯•æ¡†æ¶
æ•´åˆè§†é¢‘å¤„ç†ã€ç®—æ³•æµ‹è¯•ã€æ€§èƒ½è¯„ä¼°äºä¸€ä½“çš„å®Œæ•´æµ‹è¯•è§£å†³æ–¹æ¡ˆ
"""

import os
import json
import time
import threading
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Callable, Any
from dataclasses import dataclass, asdict
from pathlib import Path
import sqlite3

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('test_framework.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class TestCase:
    """æµ‹è¯•ç”¨ä¾‹æ•°æ®ç»“æ„"""
    id: str
    name: str
    description: str
    video_path: str
    expected_detections: List[str]  # ['fall', 'fire', 'smoke']
    difficulty: str  # easy, medium, hard, very_hard
    duration_seconds: int
    ground_truth: Optional[Dict] = None  # æ ‡æ³¨æ•°æ®
    tags: Optional[List[str]] = None

@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœæ•°æ®ç»“æ„"""
    test_case_id: str
    session_id: str
    start_time: str
    end_time: str
    detections: List[Dict]
    alerts: List[Dict]
    performance_metrics: Dict
    statistics: Dict
    passed: bool
    error_message: Optional[str] = None

class DatabaseManager:
    """æµ‹è¯•æ•°æ®åº“ç®¡ç†å™¨"""
    
    def __init__(self, db_path: str = "test_results.db"):
        self.db_path = db_path
        self.init_database()
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æµ‹è¯•ç”¨ä¾‹è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS test_cases (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                description TEXT,
                video_path TEXT,
                expected_detections TEXT,
                difficulty TEXT,
                duration_seconds INTEGER,
                ground_truth TEXT,
                tags TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # æµ‹è¯•ç»“æœè¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS test_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                test_case_id TEXT,
                session_id TEXT,
                start_time TEXT,
                end_time TEXT,
                detections TEXT,
                alerts TEXT,
                performance_metrics TEXT,
                statistics TEXT,
                passed BOOLEAN,
                error_message TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (test_case_id) REFERENCES test_cases (id)
            )
        ''')
        
        # æ€§èƒ½åŸºå‡†è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS performance_benchmarks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                test_type TEXT,
                metric_name TEXT,
                expected_value REAL,
                tolerance REAL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def save_test_case(self, test_case: TestCase):
        """ä¿å­˜æµ‹è¯•ç”¨ä¾‹"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR REPLACE INTO test_cases 
            (id, name, description, video_path, expected_detections, difficulty, 
             duration_seconds, ground_truth, tags)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            test_case.id, test_case.name, test_case.description, test_case.video_path,
            json.dumps(test_case.expected_detections), test_case.difficulty,
            test_case.duration_seconds, json.dumps(test_case.ground_truth),
            json.dumps(test_case.tags)
        ))
        
        conn.commit()
        conn.close()
    
    def save_test_result(self, result: TestResult):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO test_results 
            (test_case_id, session_id, start_time, end_time, detections, alerts,
             performance_metrics, statistics, passed, error_message)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            result.test_case_id, result.session_id, result.start_time, result.end_time,
            json.dumps(result.detections), json.dumps(result.alerts),
            json.dumps(result.performance_metrics), json.dumps(result.statistics),
            result.passed, result.error_message
        ))
        
        conn.commit()
        conn.close()
    
    def get_test_history(self, test_case_id: str, limit: int = 10) -> List[Dict]:
        """è·å–æµ‹è¯•å†å²"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT * FROM test_results 
            WHERE test_case_id = ? 
            ORDER BY created_at DESC 
            LIMIT ?
        ''', (test_case_id, limit))
        
        results = cursor.fetchall()
        conn.close()
        
        columns = ['id', 'test_case_id', 'session_id', 'start_time', 'end_time',
                  'detections', 'alerts', 'performance_metrics', 'statistics',
                  'passed', 'error_message', 'created_at']
        
        return [dict(zip(columns, row)) for row in results]

class PerformanceProfiler:
    """æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        self.metrics = {}
        self.start_times = {}
    
    def start_timer(self, metric_name: str):
        """å¼€å§‹è®¡æ—¶"""
        self.start_times[metric_name] = time.time()
    
    def end_timer(self, metric_name: str):
        """ç»“æŸè®¡æ—¶"""
        if metric_name in self.start_times:
            elapsed = time.time() - self.start_times[metric_name]
            self.metrics[metric_name] = elapsed
            del self.start_times[metric_name]
            return elapsed
        return 0
    
    def record_metric(self, metric_name: str, value: float):
        """è®°å½•æŒ‡æ ‡"""
        self.metrics[metric_name] = value
    
    def get_metrics(self) -> Dict[str, float]:
        """è·å–æ‰€æœ‰æŒ‡æ ‡"""
        return self.metrics.copy()
    
    def reset(self):
        """é‡ç½®æ‰€æœ‰æŒ‡æ ‡"""
        self.metrics.clear()
        self.start_times.clear()

class TestValidator:
    """æµ‹è¯•ç»“æœéªŒè¯å™¨"""
    
    @staticmethod
    def validate_detection_accuracy(actual_detections: List[Dict], 
                                  expected_types: List[str]) -> Dict[str, float]:
        """éªŒè¯æ£€æµ‹å‡†ç¡®æ€§"""
        detected_types = [d['type'] for d in actual_detections]
        
        # è®¡ç®—ç²¾ç¡®ç‡å’Œå¬å›ç‡
        true_positives = len(set(detected_types) & set(expected_types))
        false_positives = len(detected_types) - true_positives
        false_negatives = len(expected_types) - true_positives
        
        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        return {
            'precision': precision,
            'recall': recall,
            'f1_score': f1_score,
            'true_positives': true_positives,
            'false_positives': false_positives,
            'false_negatives': false_negatives
        }
    
    @staticmethod
    def validate_confidence_scores(detections: List[Dict]) -> Dict[str, float]:
        """éªŒè¯ç½®ä¿¡åº¦åˆ†æ•°"""
        if not detections:
            return {'avg_confidence': 0, 'min_confidence': 0, 'max_confidence': 0}
        
        confidences = [d.get('confidence', 0) for d in detections]
        return {
            'avg_confidence': sum(confidences) / len(confidences),
            'min_confidence': min(confidences),
            'max_confidence': max(confidences),
            'std_confidence': (sum([(c - sum(confidences)/len(confidences))**2 for c in confidences]) / len(confidences))**0.5
        }
    
    @staticmethod
    def validate_performance_metrics(metrics: Dict[str, float]) -> Dict[str, bool]:
        """éªŒè¯æ€§èƒ½æŒ‡æ ‡"""
        benchmarks = {
            'processing_time': 5.0,  # 5ç§’å†…å®Œæˆ
            'memory_usage': 1000.0,  # 1GBå†…å­˜é™åˆ¶
            'cpu_usage': 80.0,       # 80%CPUä½¿ç”¨ç‡é™åˆ¶
            'detection_latency': 1.0  # 1ç§’æ£€æµ‹å»¶è¿Ÿ
        }
        
        validation_results = {}
        for metric, benchmark in benchmarks.items():
            if metric in metrics:
                validation_results[f"{metric}_passed"] = metrics[metric] <= benchmark
            else:
                validation_results[f"{metric}_passed"] = True  # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œé»˜è®¤é€šè¿‡
        
        return validation_results

class TestExecutor:
    """æµ‹è¯•æ‰§è¡Œå™¨"""
    
    def __init__(self):
        self.db_manager = DatabaseManager()
        self.profiler = PerformanceProfiler()
        self.validator = TestValidator()
        
        # å¯¼å…¥æ£€æµ‹å™¨
        try:
            from real_fall_detector import SimpleFallDetector, RealFireSmokeDetector
            self.fall_detector = SimpleFallDetector()
            self.fire_detector = RealFireSmokeDetector()
            logger.info("æ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
        except ImportError as e:
            logger.error(f"æ£€æµ‹å™¨å¯¼å…¥å¤±è´¥: {e}")
            self.fall_detector = None
            self.fire_detector = None
    
    def execute_test_case(self, test_case: TestCase, 
                         callback: Optional[Callable] = None) -> TestResult:
        """æ‰§è¡Œå•ä¸ªæµ‹è¯•ç”¨ä¾‹"""
        session_id = f"test_{test_case.id}_{int(time.time())}"
        start_time = datetime.now().isoformat()
        
        logger.info(f"å¼€å§‹æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹: {test_case.name} (ID: {test_case.id})")
        
        self.profiler.reset()
        self.profiler.start_timer('total_execution_time')
        
        try:
            # æ£€æŸ¥è§†é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(test_case.video_path):
                raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {test_case.video_path}")
            
            # æ‰§è¡Œæ£€æµ‹
            detections, alerts = self._run_detection(test_case, callback)
            
            # ç»“æŸè®¡æ—¶
            self.profiler.end_timer('total_execution_time')
            
            # æ”¶é›†æ€§èƒ½æŒ‡æ ‡
            performance_metrics = self.profiler.get_metrics()
            
            # éªŒè¯ç»“æœ
            accuracy_metrics = self.validator.validate_detection_accuracy(
                detections, test_case.expected_detections
            )
            confidence_metrics = self.validator.validate_confidence_scores(detections)
            performance_validation = self.validator.validate_performance_metrics(performance_metrics)
            
            # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
            statistics = {
                'total_detections': len(detections),
                'total_alerts': len(alerts),
                'detection_types': list(set(d['type'] for d in detections)),
                'accuracy_metrics': accuracy_metrics,
                'confidence_metrics': confidence_metrics,
                'performance_validation': performance_validation
            }
            
            # åˆ¤æ–­æ˜¯å¦é€šè¿‡æµ‹è¯•
            passed = (
                accuracy_metrics['f1_score'] >= 0.5 and  # F1åˆ†æ•°è‡³å°‘50%
                all(performance_validation.values()) and  # æ€§èƒ½æŒ‡æ ‡å…¨éƒ¨é€šè¿‡
                len(detections) > 0  # è‡³å°‘æœ‰ä¸€ä¸ªæ£€æµ‹ç»“æœ
            )
            
            end_time = datetime.now().isoformat()
            
            result = TestResult(
                test_case_id=test_case.id,
                session_id=session_id,
                start_time=start_time,
                end_time=end_time,
                detections=detections,
                alerts=alerts,
                performance_metrics=performance_metrics,
                statistics=statistics,
                passed=passed
            )
            
            # ä¿å­˜ç»“æœåˆ°æ•°æ®åº“
            self.db_manager.save_test_result(result)
            
            logger.info(f"æµ‹è¯•ç”¨ä¾‹ {test_case.name} æ‰§è¡Œå®Œæˆ: {'é€šè¿‡' if passed else 'å¤±è´¥'}")
            return result
            
        except Exception as e:
            end_time = datetime.now().isoformat()
            error_message = str(e)
            
            logger.error(f"æµ‹è¯•ç”¨ä¾‹ {test_case.name} æ‰§è¡Œå¤±è´¥: {error_message}")
            
            result = TestResult(
                test_case_id=test_case.id,
                session_id=session_id,
                start_time=start_time,
                end_time=end_time,
                detections=[],
                alerts=[],
                performance_metrics=self.profiler.get_metrics(),
                statistics={},
                passed=False,
                error_message=error_message
            )
            
            # ä¿å­˜é”™è¯¯ç»“æœ
            self.db_manager.save_test_result(result)
            return result
    
    def _run_detection(self, test_case: TestCase, callback: Optional[Callable] = None) -> tuple:
        """è¿è¡Œæ£€æµ‹ç®—æ³•"""
        detections = []
        alerts = []
        
        # æ¨¡æ‹Ÿè§†é¢‘å¸§å¤„ç†ï¼ˆå®é™…åº”ç”¨ä¸­è¿™é‡Œä¼šè¯»å–çœŸå®è§†é¢‘ï¼‰
        # å‡è®¾30FPSï¼Œæ ¹æ®è§†é¢‘æ—¶é•¿è®¡ç®—æ€»å¸§æ•°
        total_frames = test_case.duration_seconds * 30
        
        self.profiler.start_timer('detection_processing')
        
        for frame_num in range(1, min(total_frames, 300) + 1):  # æœ€å¤šå¤„ç†300å¸§
            timestamp = frame_num / 30.0
            frame_data = f"{test_case.video_path}_frame_{frame_num}"
            
            # è·Œå€’æ£€æµ‹
            if 'fall' in test_case.expected_detections and self.fall_detector:
                fall_result = self.fall_detector.detect_fall_from_video(
                    frame_data, frame_num, timestamp
                )
                
                if fall_result['is_fall']:
                    detection = {
                        'type': 'fall',
                        'frame_number': frame_num,
                        'timestamp': timestamp,
                        'confidence': fall_result['confidence'],
                        'details': fall_result
                    }
                    detections.append(detection)
                    
                    # ç”Ÿæˆå‘Šè­¦
                    if fall_result['confidence'] > 0.7:
                        alert = self._create_alert('fall', frame_num, timestamp, fall_result['confidence'])
                        alerts.append(alert)
            
            # ç«ç„°çƒŸé›¾æ£€æµ‹
            if any(dt in test_case.expected_detections for dt in ['fire', 'smoke']) and self.fire_detector:
                fire_detections = self.fire_detector.detect_fire_smoke_from_video(
                    frame_data, frame_num, timestamp
                )
                
                for fire_detection in fire_detections:
                    detection = {
                        'type': fire_detection['type'],
                        'frame_number': frame_num,
                        'timestamp': timestamp,
                        'confidence': fire_detection['confidence'],
                        'details': fire_detection
                    }
                    detections.append(detection)
                    
                    # ç”Ÿæˆå‘Šè­¦
                    if fire_detection['confidence'] > 0.6:
                        alert = self._create_alert(
                            fire_detection['type'], frame_num, timestamp, fire_detection['confidence']
                        )
                        alerts.append(alert)
            
            # å›è°ƒè¿›åº¦æ›´æ–°
            if callback and frame_num % 10 == 0:
                progress = (frame_num / total_frames) * 100
                callback(progress, frame_num, detections, alerts)
            
            # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            time.sleep(0.01)
        
        self.profiler.end_timer('detection_processing')
        return detections, alerts
    
    def _create_alert(self, detection_type: str, frame_num: int, 
                     timestamp: float, confidence: float) -> Dict:
        """åˆ›å»ºå‘Šè­¦"""
        severity_map = {
            'fire': 'CRITICAL',
            'smoke': 'HIGH',
            'fall': 'HIGH' if confidence > 0.8 else 'MEDIUM'
        }
        
        return {
            'id': f"{detection_type}_{frame_num}_{int(time.time())}",
            'type': detection_type,
            'severity': severity_map.get(detection_type, 'MEDIUM'),
            'message': f"æ£€æµ‹åˆ°{detection_type}äº‹ä»¶ (å¸§:{frame_num}, ç½®ä¿¡åº¦:{confidence:.2f})",
            'timestamp': datetime.now().isoformat(),
            'frame_number': frame_num,
            'confidence': confidence
        }

class TestSuiteManager:
    """æµ‹è¯•å¥—ä»¶ç®¡ç†å™¨"""
    
    def __init__(self):
        self.test_cases = {}
        self.executor = TestExecutor()
        self.db_manager = DatabaseManager()
        
        # åˆå§‹åŒ–é¢„è®¾æµ‹è¯•ç”¨ä¾‹
        self._initialize_default_test_cases()
    
    def _initialize_default_test_cases(self):
        """åˆå§‹åŒ–é»˜è®¤æµ‹è¯•ç”¨ä¾‹"""
        default_cases = [
            TestCase(
                id="fall_001",
                name="è€äººå«ç”Ÿé—´è·Œå€’æ£€æµ‹",
                description="æ¨¡æ‹Ÿè€äººåœ¨å«ç”Ÿé—´è·Œå€’çš„åœºæ™¯ï¼Œæµ‹è¯•ç®—æ³•åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„æ£€æµ‹èƒ½åŠ›",
                video_path="test_videos/elderly_bathroom_fall.mp4",
                expected_detections=["fall"],
                difficulty="medium",
                duration_seconds=45,
                tags=["fall", "elderly", "bathroom", "medium"]
            ),
            TestCase(
                id="fall_002", 
                name="å§å®¤è·Œå€’æ£€æµ‹",
                description="æ¨¡æ‹Ÿè€äººåœ¨å§å®¤è·Œå€’çš„åœºæ™¯ï¼Œå…‰çº¿è¾ƒæš—ç¯å¢ƒä¸‹çš„æ£€æµ‹æµ‹è¯•",
                video_path="test_videos/elderly_bedroom_fall.mp4",
                expected_detections=["fall"],
                difficulty="easy",
                duration_seconds=30,
                tags=["fall", "elderly", "bedroom", "easy"]
            ),
            TestCase(
                id="fire_001",
                name="å¨æˆ¿ç«ç¾æ£€æµ‹",
                description="å¨æˆ¿ç¯å¢ƒä¸‹çš„ç«ç„°å’ŒçƒŸé›¾æ£€æµ‹ï¼ŒåŒ…å«å¤æ‚èƒŒæ™¯å¹²æ‰°",
                video_path="test_videos/kitchen_fire.mp4",
                expected_detections=["fire", "smoke"],
                difficulty="medium",
                duration_seconds=90,
                tags=["fire", "smoke", "kitchen", "medium"]
            ),
            TestCase(
                id="fire_002",
                name="å®¢å…çƒŸé›¾æ£€æµ‹",
                description="å®¢å…ç¯å¢ƒä¸‹çš„çƒŸé›¾æ£€æµ‹ï¼Œæµ‹è¯•æ—©æœŸç«ç¾é¢„è­¦èƒ½åŠ›",
                video_path="test_videos/living_room_smoke.mp4", 
                expected_detections=["smoke"],
                difficulty="easy",
                duration_seconds=60,
                tags=["smoke", "living_room", "easy"]
            ),
            TestCase(
                id="complex_001",
                name="å¤åˆåœºæ™¯æ£€æµ‹",
                description="åŒ…å«è·Œå€’å’Œç«ç¾çš„å¤æ‚ç´§æ€¥æƒ…å†µï¼Œæµ‹è¯•å¤šç±»å‹åŒæ—¶æ£€æµ‹",
                video_path="test_videos/emergency_evacuation.mp4",
                expected_detections=["fall", "fire", "smoke"],
                difficulty="very_hard",
                duration_seconds=180,
                tags=["fall", "fire", "smoke", "complex", "very_hard"]
            )
        ]
        
        for test_case in default_cases:
            self.add_test_case(test_case)
    
    def add_test_case(self, test_case: TestCase):
        """æ·»åŠ æµ‹è¯•ç”¨ä¾‹"""
        self.test_cases[test_case.id] = test_case
        self.db_manager.save_test_case(test_case)
        logger.info(f"æ·»åŠ æµ‹è¯•ç”¨ä¾‹: {test_case.name} (ID: {test_case.id})")
    
    def get_test_case(self, test_id: str) -> Optional[TestCase]:
        """è·å–æµ‹è¯•ç”¨ä¾‹"""
        return self.test_cases.get(test_id)
    
    def list_test_cases(self, difficulty: Optional[str] = None, 
                       tags: Optional[List[str]] = None) -> List[TestCase]:
        """åˆ—å‡ºæµ‹è¯•ç”¨ä¾‹"""
        cases = list(self.test_cases.values())
        
        if difficulty:
            cases = [case for case in cases if case.difficulty == difficulty]
        
        if tags:
            cases = [case for case in cases 
                    if case.tags and any(tag in case.tags for tag in tags)]
        
        return cases
    
    def run_test_suite(self, test_ids: Optional[List[str]] = None,
                      progress_callback: Optional[Callable] = None) -> Dict[str, TestResult]:
        """è¿è¡Œæµ‹è¯•å¥—ä»¶"""
        if test_ids is None:
            test_ids = list(self.test_cases.keys())
        
        results = {}
        total_tests = len(test_ids)
        
        logger.info(f"å¼€å§‹è¿è¡Œæµ‹è¯•å¥—ä»¶ï¼Œå…± {total_tests} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        
        for i, test_id in enumerate(test_ids):
            test_case = self.test_cases.get(test_id)
            if not test_case:
                logger.warning(f"æµ‹è¯•ç”¨ä¾‹ä¸å­˜åœ¨: {test_id}")
                continue
            
            # è¿è¡Œæµ‹è¯•
            result = self.executor.execute_test_case(test_case, progress_callback)
            results[test_id] = result
            
            # æ›´æ–°æ•´ä½“è¿›åº¦
            if progress_callback:
                overall_progress = ((i + 1) / total_tests) * 100
                progress_callback(overall_progress, f"å®Œæˆæµ‹è¯• {i + 1}/{total_tests}")
        
        logger.info(f"æµ‹è¯•å¥—ä»¶è¿è¡Œå®Œæˆï¼Œé€šè¿‡ç‡: {sum(1 for r in results.values() if r.passed)}/{total_tests}")
        return results
    
    def generate_report(self, results: Dict[str, TestResult]) -> Dict:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        total_tests = len(results)
        passed_tests = sum(1 for r in results.values() if r.passed)
        failed_tests = total_tests - passed_tests
        
        # æŒ‰éš¾åº¦ç»Ÿè®¡
        difficulty_stats = {}
        for test_id, result in results.items():
            test_case = self.test_cases.get(test_id)
            if test_case:
                difficulty = test_case.difficulty
                if difficulty not in difficulty_stats:
                    difficulty_stats[difficulty] = {'total': 0, 'passed': 0}
                difficulty_stats[difficulty]['total'] += 1
                if result.passed:
                    difficulty_stats[difficulty]['passed'] += 1
        
        # æŒ‰æ£€æµ‹ç±»å‹ç»Ÿè®¡
        detection_type_stats = {}
        for result in results.values():
            for detection in result.detections:
                det_type = detection['type']
                if det_type not in detection_type_stats:
                    detection_type_stats[det_type] = {'count': 0, 'avg_confidence': 0}
                detection_type_stats[det_type]['count'] += 1
        
        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        for det_type in detection_type_stats:
            confidences = []
            for result in results.values():
                confidences.extend([d['confidence'] for d in result.detections if d['type'] == det_type])
            if confidences:
                detection_type_stats[det_type]['avg_confidence'] = sum(confidences) / len(confidences)
        
        # æ€§èƒ½ç»Ÿè®¡
        processing_times = [r.performance_metrics.get('total_execution_time', 0) 
                           for r in results.values()]
        avg_processing_time = sum(processing_times) / len(processing_times) if processing_times else 0
        
        report = {
            'summary': {
                'total_tests': total_tests,
                'passed_tests': passed_tests,
                'failed_tests': failed_tests,
                'pass_rate': passed_tests / total_tests if total_tests > 0 else 0,
                'avg_processing_time': avg_processing_time
            },
            'difficulty_breakdown': difficulty_stats,
            'detection_type_breakdown': detection_type_stats,
            'failed_tests': [
                {
                    'test_id': test_id,
                    'test_name': self.test_cases.get(test_id, TestCase('', '', '', '', [], '', 0)).name,
                    'error_message': result.error_message,
                    'statistics': result.statistics
                }
                for test_id, result in results.items() 
                if not result.passed
            ],
            'generated_at': datetime.now().isoformat()
        }
        
        return report

# ä¾¿æ·å‡½æ•°
def quick_test(test_type: str = "all", difficulty: str = "easy") -> Dict:
    """å¿«é€Ÿæµ‹è¯•å‡½æ•°"""
    suite_manager = TestSuiteManager()
    
    # æ ¹æ®å‚æ•°ç­›é€‰æµ‹è¯•ç”¨ä¾‹
    test_cases = suite_manager.list_test_cases(difficulty=difficulty)
    
    if test_type != "all":
        test_cases = [case for case in test_cases 
                     if test_type in case.expected_detections]
    
    if not test_cases:
        return {"error": "æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æµ‹è¯•ç”¨ä¾‹"}
    
    # è¿è¡Œæµ‹è¯•
    test_ids = [case.id for case in test_cases]
    results = suite_manager.run_test_suite(test_ids)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = suite_manager.generate_report(results)
    
    return {
        "test_results": results,
        "report": report,
        "message": f"å¿«é€Ÿæµ‹è¯•å®Œæˆï¼Œè¿è¡Œäº† {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹"
    }

if __name__ == "__main__":
    print("ğŸ§ª è·Œå€’æ£€æµ‹æµ‹è¯•æ¡†æ¶")
    print("==================")
    
    # æ¼”ç¤ºå¿«é€Ÿæµ‹è¯•
    result = quick_test(test_type="fall", difficulty="easy")
    
    if "error" in result:
        print(f"âŒ {result['error']}")
    else:
        print(f"âœ… {result['message']}")
        print(f"ğŸ“Š é€šè¿‡ç‡: {result['report']['summary']['pass_rate']:.1%}")
        print(f"â±ï¸  å¹³å‡å¤„ç†æ—¶é—´: {result['report']['summary']['avg_processing_time']:.2f}ç§’")