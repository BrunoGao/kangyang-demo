#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºOpenCVçš„è·Œå€’æ£€æµ‹å™¨
ä½¿ç”¨è®¡ç®—æœºè§†è§‰æŠ€æœ¯è¿›è¡Œè¿åŠ¨æ£€æµ‹å’Œå½¢çŠ¶åˆ†æ
"""

import cv2
import numpy as np
import math
import time
from datetime import datetime
from typing import List, Dict, Tuple, Optional, Any

class OpenCVFallDetector:
    """åŸºäºOpenCVçš„è·Œå€’æ£€æµ‹å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ£€æµ‹å™¨"""
        # æ£€æµ‹å‚æ•°
        self.motion_threshold = 30000  # è¿åŠ¨åŒºåŸŸé˜ˆå€¼
        self.aspect_ratio_threshold = 2.5  # å®½é«˜æ¯”é˜ˆå€¼ï¼ˆèººå€’æ—¶å®½>é«˜ï¼‰
        self.area_threshold = 5000  # æœ€å°æ£€æµ‹åŒºåŸŸ
        self.fall_duration_min = 1.0   # æœ€å°è·Œå€’æŒç»­æ—¶é—´(ç§’)
        
        # èƒŒæ™¯å‡æ³•å™¨
        self.bg_subtractor = cv2.createBackgroundSubtractorMOG2(
            detectShadows=True, varThreshold=50, history=100
        )
        
        # å½¢æ€å­¦æ“ä½œæ ¸
        self.kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        
        # çŠ¶æ€è·Ÿè¸ª
        self.previous_contours = []
        self.fall_start_time = None
        self.consecutive_fall_frames = 0
        self.person_positions = []
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.detection_stats = {
            'total_frames': 0,
            'person_detected': 0,
            'fall_detected': 0,
            'false_positives': 0,
            'processing_times': []
        }
    
    def detect_fall_from_frame(self, frame: np.ndarray, timestamp: float, 
                              frame_number: int = 0) -> Dict[str, Any]:
        """
        ä»è§†é¢‘å¸§æ£€æµ‹è·Œå€’
        
        Args:
            frame: è¾“å…¥è§†é¢‘å¸§
            timestamp: æ—¶é—´æˆ³
            frame_number: å¸§ç¼–å·
            
        Returns:
            æ£€æµ‹ç»“æœå­—å…¸
        """
        start_time = time.time()
        self.detection_stats['total_frames'] += 1
        
        detection_result = {
            'frame_number': frame_number,
            'timestamp': timestamp,
            'person_detected': False,
            'is_fall': False,
            'confidence': 0.0,
            'aspect_ratio': 0.0,
            'motion_area': 0.0,
            'fall_duration': 0.0,
            'bbox': None,
            'analysis_timestamp': datetime.now().isoformat(),
            'processing_time': 0.0
        }
        
        # è¿åŠ¨æ£€æµ‹
        motion_mask = self.bg_subtractor.apply(frame)
        
        # å½¢æ€å­¦æ“ä½œå»å™ª
        motion_mask = cv2.morphologyEx(motion_mask, cv2.MORPH_OPEN, self.kernel)
        motion_mask = cv2.morphologyEx(motion_mask, cv2.MORPH_CLOSE, self.kernel)
        
        # æŸ¥æ‰¾è½®å»“
        contours, _ = cv2.findContours(motion_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        # è¿‡æ»¤å°è½®å»“
        significant_contours = [c for c in contours if cv2.contourArea(c) > self.area_threshold]
        
        if significant_contours:
            self.detection_stats['person_detected'] += 1
            detection_result['person_detected'] = True
            
            # æ‰¾åˆ°æœ€å¤§è½®å»“ï¼ˆå‡è®¾æ˜¯äººä½“ï¼‰
            largest_contour = max(significant_contours, key=cv2.contourArea)
            
            # è®¡ç®—è½®å»“å±æ€§
            area = cv2.contourArea(largest_contour)
            detection_result['motion_area'] = area
            
            # è®¡ç®—è¾¹ç•Œæ¡†å’Œå®½é«˜æ¯”
            x, y, w, h = cv2.boundingRect(largest_contour)
            detection_result['bbox'] = [x, y, x + w, y + h]
            
            aspect_ratio = w / h if h > 0 else 0
            detection_result['aspect_ratio'] = aspect_ratio
            
            # è·Œå€’åˆ¤æ–­
            is_fall, confidence, fall_details = self._determine_fall(
                aspect_ratio, area, timestamp, (x, y, w, h)
            )
            
            detection_result.update({
                'is_fall': is_fall,
                'confidence': confidence,
                **fall_details
            })
            
            if is_fall:
                self.detection_stats['fall_detected'] += 1
            
            # æ›´æ–°å†å²è®°å½•
            self._update_history((x + w//2, y + h//2), timestamp)
        
        # è®¡ç®—å¤„ç†æ—¶é—´
        processing_time = time.time() - start_time
        detection_result['processing_time'] = round(processing_time, 4)
        self.detection_stats['processing_times'].append(processing_time)
        
        return detection_result
    
    def _determine_fall(self, aspect_ratio: float, area: float, timestamp: float,
                       bbox: Tuple[int, int, int, int]) -> Tuple[bool, float, Dict]:
        """åˆ¤æ–­æ˜¯å¦å‘ç”Ÿè·Œå€’"""
        fall_details = {
            'fall_duration': 0.0,
            'severity': 'NONE',
            'fall_direction': 'unknown',
            'body_position': 'upright'
        }
        
        # è·Œå€’åˆ¤æ–­æ¡ä»¶
        horizontal_body = aspect_ratio > self.aspect_ratio_threshold  # èº«ä½“æ°´å¹³
        significant_motion = area > self.motion_threshold  # æ˜¾è‘—è¿åŠ¨
        
        # ä½ç½®å˜åŒ–æ£€æµ‹
        position_change = self._analyze_position_change(bbox, timestamp)
        
        # ç»¼åˆåˆ¤æ–­
        fall_conditions = [
            horizontal_body,
            significant_motion,
            position_change > 50,  # ä½ç½®æ˜¾è‘—å˜åŒ–
            len(self.person_positions) > 3  # éœ€è¦ä¸€å®šçš„å†å²æ•°æ®
        ]
        
        is_potential_fall = sum(fall_conditions) >= 2
        
        if is_potential_fall:
            if self.fall_start_time is None:
                self.fall_start_time = timestamp
            self.consecutive_fall_frames += 1
            
            # æ›´æ–°è·Œå€’è¯¦ç»†ä¿¡æ¯
            fall_duration = timestamp - (self.fall_start_time or timestamp)
            fall_details.update({
                'fall_duration': round(fall_duration, 2),
                'severity': 'HIGH' if aspect_ratio > 3.0 else 'MEDIUM',
                'fall_direction': self._determine_fall_direction(bbox),
                'body_position': 'falling' if fall_duration < 2.0 else 'down'
            })
        else:
            # é‡ç½®è·Œå€’çŠ¶æ€
            if self.consecutive_fall_frames > 0:
                fall_duration = timestamp - (self.fall_start_time or timestamp)
                if fall_duration < self.fall_duration_min:
                    self.detection_stats['false_positives'] += 1
            
            self.fall_start_time = None
            self.consecutive_fall_frames = 0
        
        # ç¡®è®¤è·Œå€’ï¼šè¿ç»­æ£€æµ‹ä¸”æŒç»­æ—¶é—´è¶³å¤Ÿ
        confirmed_fall = (
            is_potential_fall and 
            self.consecutive_fall_frames >= 5 and
            self.fall_start_time is not None and
            (timestamp - self.fall_start_time) >= self.fall_duration_min
        )
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = 0.0
        if confirmed_fall:
            confidence = 0.6  # åŸºç¡€ç½®ä¿¡åº¦
            if aspect_ratio > 3.0:
                confidence += 0.2
            if area > self.motion_threshold * 2:
                confidence += 0.1
            if self.consecutive_fall_frames > 10:
                confidence += 0.1
            confidence = min(0.95, confidence)
        
        return confirmed_fall, confidence, fall_details
    
    def _analyze_position_change(self, bbox: Tuple[int, int, int, int], 
                               timestamp: float) -> float:
        """åˆ†æä½ç½®å˜åŒ–"""
        x, y, w, h = bbox
        center_x, center_y = x + w//2, y + h//2
        
        if not self.person_positions:
            return 0.0
        
        # è®¡ç®—ä¸æœ€è¿‘ä½ç½®çš„è·ç¦»
        last_pos = self.person_positions[-1]
        distance = math.sqrt((center_x - last_pos['x'])**2 + (center_y - last_pos['y'])**2)
        
        return distance
    
    def _determine_fall_direction(self, bbox: Tuple[int, int, int, int]) -> str:
        """ç¡®å®šè·Œå€’æ–¹å‘"""
        x, y, w, h = bbox
        center_x = x + w//2
        
        if len(self.person_positions) >= 2:
            prev_center_x = self.person_positions[-1]['x']
            if center_x > prev_center_x + 20:
                return 'right'
            elif center_x < prev_center_x - 20:
                return 'left'
        
        return 'forward'
    
    def _update_history(self, position: Tuple[int, int], timestamp: float):
        """æ›´æ–°å†å²è®°å½•"""
        center_x, center_y = position
        
        # æ·»åŠ åˆ°å†å²è®°å½•
        self.person_positions.append({
            'timestamp': timestamp,
            'x': center_x,
            'y': center_y
        })
        
        # ä¿æŒå†å²è®°å½•é•¿åº¦
        max_history = 30
        if len(self.person_positions) > max_history:
            self.person_positions = self.person_positions[-max_history:]
    
    def get_detection_statistics(self) -> Dict:
        """è·å–æ£€æµ‹ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.detection_stats.copy()
        
        if stats['total_frames'] > 0:
            stats['person_detection_rate'] = stats['person_detected'] / stats['total_frames']
            stats['fall_detection_rate'] = stats['fall_detected'] / stats['total_frames']
            stats['false_positive_rate'] = stats['false_positives'] / max(1, stats['fall_detected'])
        else:
            stats['person_detection_rate'] = 0.0
            stats['fall_detection_rate'] = 0.0
            stats['false_positive_rate'] = 0.0
        
        if stats['processing_times']:
            stats['avg_processing_time'] = sum(stats['processing_times']) / len(stats['processing_times'])
            stats['max_processing_time'] = max(stats['processing_times'])
            stats['min_processing_time'] = min(stats['processing_times'])
        else:
            stats['avg_processing_time'] = 0.0
            stats['max_processing_time'] = 0.0
            stats['min_processing_time'] = 0.0
        
        return stats
    
    def reset_detection_state(self):
        """é‡ç½®æ£€æµ‹çŠ¶æ€"""
        self.fall_start_time = None
        self.consecutive_fall_frames = 0
        self.person_positions.clear()
        self.previous_contours.clear()
        
        # é‡ç½®èƒŒæ™¯å‡æ³•å™¨
        self.bg_subtractor = cv2.createBackgroundSubtractorMOG2(
            detectShadows=True, varThreshold=50, history=100
        )
    
    def update_parameters(self, **kwargs):
        """æ›´æ–°æ£€æµ‹å‚æ•°"""
        if 'motion_threshold' in kwargs:
            self.motion_threshold = kwargs['motion_threshold']
        if 'aspect_ratio_threshold' in kwargs:
            self.aspect_ratio_threshold = kwargs['aspect_ratio_threshold']
        if 'area_threshold' in kwargs:
            self.area_threshold = kwargs['area_threshold']
        if 'fall_duration_min' in kwargs:
            self.fall_duration_min = kwargs['fall_duration_min']
    
    def draw_detection_on_frame(self, frame: np.ndarray, detection_result: Dict) -> np.ndarray:
        """åœ¨å¸§ä¸Šç»˜åˆ¶æ£€æµ‹ç»“æœ"""
        annotated_frame = frame.copy()
        
        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        if detection_result.get('bbox'):
            bbox = detection_result['bbox']
            color = (0, 0, 255) if detection_result['is_fall'] else (0, 255, 0)
            cv2.rectangle(annotated_frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color, 2)
            
            # æ·»åŠ æ ‡ç­¾
            label = f"Fall: {detection_result['is_fall']}"
            if detection_result['is_fall']:
                label += f" ({detection_result['confidence']:.2f})"
            
            cv2.putText(annotated_frame, label, (bbox[0], bbox[1] - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
        
        return annotated_frame

class OpenCVVideoProcessor:
    """åŸºäºOpenCVçš„è§†é¢‘å¤„ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¤„ç†å™¨"""
        self.detector = OpenCVFallDetector()
        self.processing_stats = {
            'total_videos': 0,
            'total_frames': 0,
            'total_falls': 0,
            'processing_time': 0.0
        }
    
    def process_video(self, video_path: str, output_path: str = None, 
                     detection_interval: int = 1) -> Dict:
        """
        å¤„ç†è§†é¢‘è¿›è¡Œè·Œå€’æ£€æµ‹
        
        Args:
            video_path: è¾“å…¥è§†é¢‘è·¯å¾„
            output_path: è¾“å‡ºè§†é¢‘è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            detection_interval: æ£€æµ‹é—´éš”ï¼ˆå¸§ï¼‰
            
        Returns:
            æ£€æµ‹ç»“æœ
        """
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
        
        # è·å–è§†é¢‘ä¿¡æ¯
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        # å‡†å¤‡è¾“å‡ºè§†é¢‘ï¼ˆå¦‚æœéœ€è¦ï¼‰
        out = None
        if output_path:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        fall_events = []
        frame_results = []
        frame_count = 0
        start_time = time.time()
        
        print(f"å¼€å§‹å¤„ç†è§†é¢‘: {video_path}")
        print(f"æ€»å¸§æ•°: {total_frames}, FPS: {fps}")
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                timestamp = frame_count / fps
                
                # æŒ‰é—´éš”è¿›è¡Œæ£€æµ‹
                if frame_count % detection_interval == 0:
                    result = self.detector.detect_fall_from_frame(frame, timestamp, frame_count)
                    frame_results.append(result)
                    
                    # è®°å½•è·Œå€’äº‹ä»¶
                    if result['is_fall']:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„è·Œå€’äº‹ä»¶
                        is_new_event = True
                        if fall_events:
                            last_event = fall_events[-1]
                            time_diff = timestamp - last_event['timestamp']
                            if time_diff < 5.0:  # 5ç§’å†…è®¤ä¸ºæ˜¯åŒä¸€äº‹ä»¶
                                is_new_event = False
                                # æ›´æ–°ç°æœ‰äº‹ä»¶
                                last_event['end_timestamp'] = timestamp
                                last_event['end_frame'] = frame_count
                                last_event['confidence'] = max(last_event['confidence'], result['confidence'])
                        
                        if is_new_event:
                            fall_event = {
                                'id': f'fall_{len(fall_events) + 1}',
                                'start_frame': frame_count,
                                'end_frame': frame_count,
                                'timestamp': timestamp,
                                'end_timestamp': timestamp,
                                'confidence': result['confidence'],
                                'aspect_ratio': result['aspect_ratio'],
                                'bbox': result['bbox'],
                                'severity': result.get('severity', 'UNKNOWN'),
                                'fall_direction': result.get('fall_direction', 'unknown')
                            }
                            fall_events.append(fall_event)
                            print(f"æ£€æµ‹åˆ°è·Œå€’äº‹ä»¶ #{len(fall_events)} åœ¨å¸§ {frame_count} (æ—¶é—´: {timestamp:.2f}s)")
                    
                    # ç»˜åˆ¶æ£€æµ‹ç»“æœï¼ˆå¦‚æœéœ€è¦è¾“å‡ºè§†é¢‘ï¼‰
                    if out:
                        annotated_frame = self.detector.draw_detection_on_frame(frame, result)
                        out.write(annotated_frame)
                elif out:
                    out.write(frame)
                
                frame_count += 1
                
                # æ˜¾ç¤ºè¿›åº¦
                if frame_count % 100 == 0:
                    progress = (frame_count / total_frames) * 100
                    print(f"å¤„ç†è¿›åº¦: {progress:.1f}% ({frame_count}/{total_frames})")
        
        finally:
            cap.release()
            if out:
                out.release()
        
        processing_time = time.time() - start_time
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.processing_stats['total_videos'] += 1
        self.processing_stats['total_frames'] += frame_count
        self.processing_stats['total_falls'] += len(fall_events)
        self.processing_stats['processing_time'] += processing_time
        
        print(f"è§†é¢‘å¤„ç†å®Œæˆ!")
        print(f"å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
        print(f"æ£€æµ‹åˆ° {len(fall_events)} ä¸ªè·Œå€’äº‹ä»¶")
        
        return {
            'video_info': {
                'path': video_path,
                'total_frames': total_frames,
                'fps': fps,
                'duration': total_frames / fps,
                'resolution': f'{width}x{height}'
            },
            'processing_info': {
                'processing_time': round(processing_time, 2),
                'processed_frames': frame_count,
                'detection_interval': detection_interval,
                'avg_fps': round(frame_count / processing_time, 2)
            },
            'detection_results': {
                'total_fall_events': len(fall_events),
                'fall_events': fall_events,
                'frame_results': frame_results
            },
            'statistics': self.detector.get_detection_statistics()
        }
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        pass

if __name__ == "__main__":
    # æµ‹è¯•OpenCVè·Œå€’æ£€æµ‹å™¨
    print("ğŸ§ª æµ‹è¯•OpenCVè·Œå€’æ£€æµ‹å™¨")
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    test_frame = np.zeros((480, 640, 3), dtype=np.uint8)
    
    detector = OpenCVFallDetector()
    
    # æµ‹è¯•æ£€æµ‹
    result = detector.detect_fall_from_frame(test_frame, 0.0, 0)
    print(f"æµ‹è¯•ç»“æœ: {result}")
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    stats = detector.get_detection_statistics()
    print(f"æ£€æµ‹ç»Ÿè®¡: {stats}")
    
    print("âœ… æµ‹è¯•å®Œæˆ")